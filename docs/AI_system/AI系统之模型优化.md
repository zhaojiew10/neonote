图优化部分，它包含了算子融合、布局转换、算子替换、内存优化等关键操作。理解这个架构，才能明白图优化在整个系统中的位置和作用。在进行离线优化时，我们主要面临四大挑战。

- 结构冗余，比如模型里有无用的计算节点，或者重复的计算子图，这些都可以在不改变输出的前提下移除。
- 精度冗余，比如用FP32浮点数表示，但很多时候精度要求没那么高，或者数据里有大量重复的零值，这些都是浪费。
- 算法冗余，比如某些算子的实现方式本身就存在重复计算
- 读写冗余，比如内存访问不连续，导致缓存效率低下。

这些都是我们需要优化的目标。针对这些冗余，我们有相应的应对策略。

- 对结构冗余，我们有算子融合、算子替换、常量折叠等手段。算子融合就像把几个连续的小步骤合并成一个大步骤，减少中间数据传输，特别适合GPU。常量折叠，就是把编译时就能确定的计算结果直接存起来，运行时直接用，省时省力。
- 对精度冗余，量化、稀疏化、低秩近似是常用方法，把数据压缩到更小的精度或格式，减少存储和计算。
- 对于算法和读写冗余，我们则通过统一表达方式、优化数据布局和内存管理来解决。这些都是离线优化的核心武器库。

这里需要区分一下，AI框架的训练优化，通常在训练阶段进行，时间相对充裕，可以进行复杂的图分析和编译。而推理引擎的优化，更多是基于预设的模板进行转换，目的是快速减少计算图中的冗余。虽然这种方法也能带来显著收益，但它的局限在于，优化效果很大程度上取决于预设模板的覆盖范围，对于非常复杂的模型，可能无法完全消除所有冗余。

在具体的优化方法上，可以分为两大类：基础优化和扩展优化。

- 基础优化，顾名思义，是最基本、最通用的优化手段，比如我们之前提到的常量折叠、冗余节点消除、以及有限数量的算子融合。这些操作通常保留了计算图的原始语义，只是让运行更高效。
- 扩展优化，则更加针对性，特别是针对特定硬件平台，比如GPU的算子融合，或者不同框架间数据格式如NCHW、NHWC的布局转换，这些都属于扩展优化的范畴。

ONNX Runtime。它最大的特点就是跨平台，能在各种硬件上跑，而且对ONNX格式的支持非常好。ORT提供了多种优化方向，比如计算流优化、数据布局转换、量化相关优化、格式转换等等。实现这些优化的核心是几个关键接口，比如GraphTransformer和RewriteRule，它们是构建各种优化策略的基础。

```py
import onnxruntime

sess_options = onnxruntime.SessionOptions()

# 开启图优化
sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL

# 加载模型
session = onnxruntime.InferenceSession("model.onnx", sess_options)

# 执行推理
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name
result = session.run([output_name], {input_name: dummy_input.numpy()})
```

深入到具体的优化策略。为什么要费劲地优化这些模型？原因很简单：现在的模型太“胖”了！计算量巨大，动不动就几百亿、上千亿参数，推理起来慢吞吞，显存不够用，部署困难。优化的目标很明确：

- 让模型跑得更快，推理时间更短；
- 让模型吃得更少，显存占用更低，这样就能支持更大、更复杂的模型；
- 让模型结构更清晰，方便我们理解和调试。

## 各种优化方案

### 冗余节点消除

在计算图里，也经常出现这种“先加后减”或者“先转后转回”的操作。比如，先用Squeeze把维度压缩，再用ExpandDims把维度展开，还有Cast、Quant、Dequant这种来回转换，或者Concat和Slice组合起来，如果中间没有其他操作，完全可以简化。识别并删除这些无用的节点，就像清理掉代码里的冗余代码，能让模型结构更简洁，计算更高效。

### 公共子图优化

简单说，就是你模型里是不是有两块地方，它们长得一模一样，用的参数也一样，输入的也一样，只是名字可能不一样？比如，你有两个分支，都用了相同的卷积层、池化层组合，处理了同一个输入。那何必算两次呢？只需要算一遍，把结果存起来，后面再用的时候直接拿来用就行。用一个MAP表来记录已经算过的子图，下次遇到相同的，就直接复用，避免重复劳动。

### 算子融合

这就像把几个工序合并成一个流水线，效率大大提升。在GPU上，内存访问，尤其是读写HBM**(High Bandwidth Memory)**，是相当耗时的。你想想，一个简单的卷积加BN**(Batch Normalization)**加激活，如果分开算，就是先算卷积，结果存到内存里，然后读出来，再算BN，再存，再读，再算激活，中间来回折腾好几次。但如果把它们融合成一个复合算子，比如Conv-BN-Act，数据在内存里流动的次数就大大减少了，甚至可能只在SRAM里完成，完全不碰HBM。这不仅能减少访存开销，还能让GPU的计算单元更忙，提高计算效率。当然，融合的前提是数学上等价，不能瞎融合。

- Conv + BN + Act：Conv Op 后跟着的 Batch Normal 的算子可以把 BN 的参数融合到 Conv 里面
- Conv + Bias + Add：Conv Op 后跟着的 Add 可以融合到 Conv 里的 Bias 参数里面
- Conv + Scale + Act：Conv Op 后跟着的 Scale 可以融合到 Conv 里的 Weight 里面

### 算子替换

你用的某个算子，虽然功能实现了，但计算量很大，或者你的硬件比如GPU对它不太友好。这时候，我们能不能找到一个功能差不多，但更高效、更省资源的算子来替代它？这就是算子替换。比如，把两个连续的卷积层合并成一个等效的卷积层，或者用深度可分离卷积替换标准卷积，都能大幅减少计算量。当然，替换的前提是，替换后的模型性能不能有明显下降，最好能保证功能上的等价。替换的方式有很多种。最简单的是“一换一”，就是把一个算子换成另一个。

- MatMul -> Conv2D：将矩阵乘变成 Conv，因为一般框架对 Conv 是做了更多的优化
- Linear -> Conv2D：将全连接层转变成 1x1 Conv，因为对 Conv 做了更多的优化
- Batch Normal -> Scale：BN 是等价于 Scale Op 的，转换成 Scale 计算量更少，速度更快
- pReLU -> Leaky ReLU：将 pReLU 转变成 Leaky ReLU，不影响性能和精度的前提下，聚焦有限算法
- Conv -> Linear After global pooling：在 Global Pooling 之后 Conv 算子转换成为全连接层

除了“一换一”，还有“一换多”。有些算子在某些框架里可能没有单独实现，但可以用其他几个基础算子组合起来实现,能减少推理引擎需要单独实现及支持 Op 数量。

### 算子前移

这个策略有点像提前规划，把一些计算步骤挪到前面去做，这样可以避免后续的重复计算或者让计算过程更顺畅。比如，如果一个计算结果是固定的，无论输入是什么，结果都不会变，那我们就可以把这部分计算提前做出来，然后在需要的时候直接拿来用，不用每次都重新计算。或者，利用一些数学上的性质，比如交换律、结合律，把某些操作提前，或者改变顺序，让计算过程更高效。当然，前移也不是随便移的，必须保证数据依赖关系正确，不能影响最终结果。

## [Flash Attention](https://chenzomi12.github.io/04Inference05Optimize/03Extend.html#flash-attention)

现在我们来聊聊一个非常热门且重要的技术：Flash Attention。Transformer模型现在是NLP领域的绝对主力，但在处理长序列时，性能瓶颈非常明显。为什么？因为它的核心机制Attention

具体来说，Attention 的计算过程可以分为以下几个步骤：

1. 线性变换：对输入序列进行线性变换，得到 Q、K、V 三个矩阵。假设每个 token 的 embedding 维度为 k，则该步骤的复杂度为 O(n * k * 3d)。
2. 计算相似度得分：通过 Q、K 两个矩阵计算相似度得分，得到注意力权重矩阵。注意力权重矩阵的大小为 n * n，计算该矩阵的时间复杂度为 O(n^2 * d * h)。
3. 加权求和：将注意力权重矩阵与 V 矩阵相乘并加权求和，得到最终输出。该步骤的复杂度为 O(n * d * h)。

Attention 的总计算复杂度为 O(n^2 * d * h)，约为 O(n^2）时间复杂度。如果序列长度翻倍，计算量和内存占用就变成原来的四倍。

标准 Attention 的计算访存：

1. 首先，从 HBM 中读取完整的 Q 和 K 矩阵（每个大小为 N x d），计算点积得到相似度得分 S（大小为 N x N），需要进行 O(Nd + N^2)次 HBM 访问。
2. 其次，计算注意力权重 P（大小为 N x N）时，需要对 S 进行 softmax 操作，这需要进行 O(N^2)次 HBM 访问。
3. 最后，将注意力权重 P 和值向量 V（每个大小为 N x d）加权求和得到输出向量 O（大小为 N x d）时，需要进行 O(Nd)次 HBM 访问。

标准 Attention 算法的总 HBM 访问次数为 O(Nd + N^2)。当 N 比较大时，总的 HBM 访问次数可能会比较昂贵。可以从减少 HBM 的访问进行优化，而之所以存在大量的访存 HBM，一个原因是在 Attention 的计算中存在三个 kernel，每个 kernel 的计算过程都存在从 HBM 读取数据，计算完成后还要写回 HBM。如果我们将三个 Kernel 融合为一个，则就可以减少部分的访问 HBM 的次数。同时要保证在计算过程中要尽量的利用 SRAM 进行计算，避免访问 HBM 操作

Flash Attention是如何解决这个问题的呢？它的核心思路非常经典，就是**利用GPU的内存层次结构**。我们知道，SRAM速度快，但容量小；HBM带宽适中，容量大；DRAM带宽最慢，容量最大。Flash Attention的核心思想是：**尽量把计算放在SRAM这个高速缓存里进行，减少对HBM的访问**。

![flashAttention](https://chenzomi12.github.io/_images/03Extend03.png)

具体怎么做呢？主要有两个关键点：

- Tiling，也就是分块。把 NxN softmax/scores 矩阵，切成一个个小块，比如B_r乘B_c，让每个小块都能塞进SRAM里。
- Recomputation（以计算能力为代价来节省存储空间），也就是重计算。在反向传播时，为了节省内存，它会把一些中间结果丢掉，需要的时候再重新计算一遍。

Flash Attention的核心思想就把计算密集的部分，都尽量放在SRAM里完成。只有当数据量太大，塞不进SRAM的时候，才需要访问HBM。通过这种方式，Flash Attention有效地降低了对HBM的依赖，从而提升了整体性能。

## Layout和Memory优化

Layout优化主要解决的是数据格式不匹配的问题。比如，一个层的输出是NCHW格式，下一个层需要NHWC格式，怎么办？最简单的方式就是在中间插入一个Transpose算子，把格式转换过来。这样做的好处是，模型图更简洁，避免了不必要的转换。

Memory优化方面，有两个重要的概念。

- 一个是Inplace Operation，也就是原地操作。如果一个操作是element-wise的，比如加法、乘法，而且它的输入不再需要了，那么就可以直接在输入的内存上进行修改，覆盖掉旧的数据，而不需要重新分配内存。另
- 一个是Memory Sharing，也就是内存共享。如果两个数据块大小相同，而且前面一个数据块的生命周期已经结束，那么就可以让后面一个数据块直接覆盖到前面一个数据块的内存上。这些都能有效地减少内存分配和回收的开销，提高内存使用效率。
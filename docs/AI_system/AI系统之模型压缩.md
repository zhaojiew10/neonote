https://chenzomi12.github.io/04Inference03Slim/02Quant.html

模型压缩就是三个目标：

- 省钱，减少模型占用的显存
- 提速，减少计算量，特别是乘法和加法，这些是计算密集型操作，压缩了就能跑得更快
- 保质，尽量减少压缩带来的精度损失，不能为了快就牺牲了模型的准确性。

这三者之间往往需要权衡，找到那个最佳平衡点。模型压缩领域有四大金刚：**量化、剪枝、蒸馏和低秩分解**。

- 量化，就是把模型参数从高精度的浮点数，比如FP32，变成低精度的整数，比如INT8，直接砍掉存储空间和计算量。
- 剪枝，直接把那些不重要的连接或者参数给消除，模型瘦身。
- 知识蒸馏，让一个大模型教一个小模型，让小模型学到精髓，性能也上去了。
- 低秩分解，是把一个大矩阵拆分成几个小矩阵的乘积，同样能达到参数和计算量的双重削减。

模型压缩通常发生在哪个环节呢？

![模型压缩流程](https://chenzomi12.github.io/_images/01Introduction015.png)

模型训练完成，性能测试通过了，但还没部署到生产环境之前，这个阶段就是压缩的黄金时间。压缩之后，再进行部署准备，最终才能跑起来。

这些技术在哪儿能用上？最典型的移动端，手机内存、算力有限，压缩模型才能流畅运行。物联网设备，比如智能手表、传感器，资源更紧张，压缩是刚需。还有在线服务系统，像推荐、搜索，需要处理海量数据，压缩能提高吞吐量，降低成本。现在大模型火了，但参数量巨大，压缩技术能让它们在更多设备上跑起来。还有自动驾驶，实时性要求极高，压缩模型是保证安全性和效率的关键。

## 低比特量化

量化，本质上就是**用更少的比特来表示数值**。

![不同精度表示](https://chenzomi12.github.io/_images/02Quant01.png)

从32位浮点数到8位整数，位数大大减少，但能表示的范围也缩小了。量化的目的很明确：减小模型大小，降低内存占用，加速推理速度。精度损失是不可避免的，尤其是在追求极致压缩的时候，比如降到4比特甚至更低。

为什么神经网络特别适合量化？这跟它的特点有关。

- 参数量巨大，不像传统机器学习模型那么精简，量化带来的相对影响就小了。
- 计算量大，尤其是卷积运算，量化后能显著提升计算速度。
- 内存占用大，量化直接就能省下不少存储空间。
- 神经网络通常对噪声有一定鲁棒性，量化引入的量化误差，相当于给模型加了一点小噪声，只要控制得好，对最终精度的影响并不像想象中那么大。

量化的好处实实在在。最直观的就是加速，用INT8计算，相比FP32，速度提升3倍甚至更多，这在硬件支持的情况下非常明显。其次，精度损失是可控的，尤其是在图像分类这种任务上，量化后精度下降往往很小。内存节省也很明显，模型大小直接缩小，传输更快。最后，还能省电，减少访存，芯片面积也小了，这对移动设备和嵌入式系统来说至关重要。

当然，量化也不是万能的，落地时会遇到不少挑战。

- 精度问题，量化方法本身，比如线性量化，它假设数据分布是线性的，但现实数据往往不是完美的线性分布，这就会引入误差。
- 量化位数越低，损失越大，这是个硬约束。
- 任务越复杂，比如目标检测、分割，对精度要求越高，量化就越容易出问题。
- 模型本身太小，量化后可能精度损失就更明显了。

除了精度，硬件和软件也是关键。

- 硬件支持程度直接影响量化效果。比如，有的GPU支持INT8 Tensor Core，有的可能不支持，或者支持的计算方式不一样，比如是INT8还是FP8。
- 软件方面，混合精度量化，比如FP32加INT8，虽然能加速，但中间的转换、Cast操作会增加计算开销。另外模型参数量少了，但运行时内存占用不一定就少，因为中间结果也需要存储。

## 量化方法

量化方法主要有两大流派：**量化训练和训练后量化**。

- 量化训练**(Quant Aware Training, QAT)**，**在训练过程中就把量化考虑进去，让模型自己适应量化带来的误差**，精度通常会更好，但代价是训练时间会变长，需要额外的数据。
- 训练后量化 **(Post Training Quantization**），在模型训练完之后直接进行量化**，不需要重新训练，速度快，但精度损失可能会相对大一些。

我们再来对比一下QAT和PTQ。

| PTQ                    | QAT                                                    |
| ---------------------- | ------------------------------------------------------ |
| 通常较快               | 较慢                                                   |
| 无需重新训练模型       | 需要训练/微调模型                                      |
| 量化方案即插即用       | 量化方案即插即用（需要重新训练）                       |
| 对模型最终精度控制较少 | 对最终精度控制更多，因为量化参数是在训练过程中学习到的 |

选择哪种方法，取决于你的任务需求和资源限制。如果对精度要求极高，比如目标检测，那就得用QAT；如果只是想快速部署一个分类模型，PTQ可能就够了。

### 模型量化映射方法

量化具体怎么把数据压缩到8位整数呢？核心是**建立一个映射关系，把连续的浮点数范围映射到有限的整数级**。主要有两种方式：非饱和和饱和。

- 非饱和量化，简单粗暴，找到数据的最大绝对值，把它直接映射到INT8的范围正负127。

- 饱和量化就更精细一点，它会先用一个阈值比如K L散度来确定一个合适的范围，超出这个范围的值就直接截断到边界，这样能更好地保留数据的动态范围，理论上精度损失会更小。

### 模型量化操作

量化可以分为线性量化和非线性量化，目前主流的方法是线性量化。线性量化可以分为对称量化和非对称量化。

浮点与定点数据的转换公式如下：
$$
Q = \frac{R}{S} + Z
$$

$$
R = (Q - Z) * S
$$

其中，R 表示输入的浮点数据，Q 表示量化之后的定点数据，Z 表示零点（Zero Point）的数值，S 表示缩放因子（Scale）的数值。理解这个公式，就是理解了量化的基本原理。**对称量化，是线性量化的一种特殊情况，它的特点是零点 Z等于0。**

<img src="https://chenzomi12.github.io/_images/02Quant08.png" alt="对称量化" style="zoom:50%;" />

最常用的方法就是最大绝对值量化，简称abs_max。它找到所有数据的最大绝对值，然后把数据范围从负的最大值到最大值映射到负127到127。这个方法简单直观，但缺点是如果数据分布很偏，或者有极端值，可能会导致大部分数据都被压缩到很小的范围，造成信息损失。所以，它通常只适用于数据分布比较对称的情况。

非对称量化，也叫零点量化，是更通用的线性量化方法。它的核心思想是，通过引入一个零点 Z，**把数据的范围更好地映射到整个量化区间**，比如负128到127或者0到255，而不是像对称量化那样只映射到中间的对称部分。这样做的好处是，即使数据分布不对称，也能充分利用整个量化范围，从而减小量化误差。计算上稍微复杂一点，需要计算scale和offset，但通常能获得更好的精度。

## 感知量化训练 QAT

核心思想是**在训练的时候，就让模型提前感受一下量化后的效果**，这样在真正量化时，模型就能更好地适应。

- 拿到一个预训练好的模型，往模型里插入一些伪量化节点（FakeQuant），这些节点模拟了量化操作，但不会真的改变权重，只是在计算时引入了类似量化后的误差
- 用训练数据对这个带了伪量化的模型进行微调，让它学习如何适应这种误差。
- 把伪量化节点去掉，得到一个真正的量化模型。

<img src="https://chenzomi12.github.io/_images/03QAT01.png" alt="感知量化训练的步骤" style="zoom:50%;" />

这个“伪量化”节点是干嘛的？它就像一个“模拟器”，在训练时，它会把输入的数据先“假装”量化成低精度的，然后进行计算，再“假装”反量化回来，继续后面的计算。这样做的好处是，模型在训练过程中，就能感受到量化带来的“噪声”，并且通过反向传播，学习到如何调整自己的参数，来抵消这种噪声。

这些伪量化节点通常放在卷积层、全连接层、激活函数前后，确保模型的关键部分都经历了这种“量化预演”。在正向传播时，伪量化节点会把数据量化成INT8，然后计算，再反量化回FP32。

到了反向传播，梯度要传回权重，这里有个问题：量化是离散的，对梯度求导是0，梯度就传不过去了。怎么办？用一个叫直通估计器的技巧，简单粗暴地假设量化操作的梯度是1，让梯度直接通过。这样，模型就能学习到量化误差，同时又能在训练时用高精度的FP32权重。

在卷积或全连接层后通常会加入批量归一化操作（Batch Normalization），以归一化输出数据。Conv 和 BN 两个算子在正向传播时可以融合为一个算子，该操作称为 BN 折叠。在QAT中，我们也要在训练时模拟这个过程。具体来说，就是把BN的参数gamma、beta、均值、方差应用到卷积层的权重和偏置上，得到一个等效的、融合了BN的权重和偏置，然后再对这个融合后的权重进行量化。这样，训练出来的模型就能很好地适应最终部署时的BN折叠优化，推理速度更快。

<img src="https://chenzomi12.github.io/_images/03QAT06.png" alt="BN 折叠" style="zoom: 67%;" />

做QAT也有一些实用技巧。

- 不要从零开始训练，最好是从一个已经做过PTQ校准的模型开始，这样起点更高，收敛更快。
- 微调时间通常不需要太长，大概占原始训练时间的10%左右就行。
- 学习率策略很重要，推荐用余弦退火，从一个很小的值比如初始学习率的1%开始，逐步降到更低，这样可以更好地稳定STE近似，避免模型震荡。
- 优化器方面，推荐使用带动量的SGD，而不是常用的ADAM或RMSProp，因为它们的梯度缩放可能会干扰量化过程。

训练好了QAT模型，怎么在推理端跑起来呢？TensorRT是个强大的工具，它能显著加速推理。从TensorRT 8.0开始，它可以直接加载包含QAT信息的ONNX模型。

- 先用QAT训练好模型，然后把它转换成ONNX格式。
- 转换过程中，TensorRT会自动识别出那些伪量化节点，把它分解成真正的量化和反量化节点。
- 用TensorRT的优化引擎，就能生成一个高效的推理引擎。

TensorRT在处理QAT模型时，会做很多优化来榨干性能。比如常量折叠，它会把权重的量化操作直接合并到权重本身，这样推理时，权重直接就是INT8的，不需要再做FP32到INT8的转换，效率很高。还有操作融合，它会把量化、反量化、卷积、ReLU这些操作合并成一个更高效的内核，比如QConvRelu，这样就能实现真正的INT8计算，中间数据转换减少了，速度自然就上去了。

## 训练后量化 PTQ

训练后量化又分为动态和静态两种。

- 动态PTQ，主要只量化权重，激活量化的因子是运行时根据输入数据动态确定的，这种方法比较简单，但精度通常不如静态和QAT。
- 静态PTQ，也叫校准量化，它会先用一小部分数据，比如几百张图片，跑一遍模型，收集每一层激活值的统计信息，然后根据这些信息，计算出一个固定的量化因子，比如scale和offset。这个因子在整个模型中是固定的，推理时就直接用这个因子来量化数据。这种方法速度快，精度损失也相对可控。

静态PTQ的流程是这样的

- 先加载一个预训练好的模型，然后准备一小批校准数据，比如500到1000张图片。用这些数据跑一遍模型，收集每一层激活值的统计信息，比如最大值、最小值，或者直方图。
- 根据这些统计信息，计算出每个层需要的量化因子，比如scale和offset。
- 把FP32的模型转换成INT8模型，把算出来的量化因子应用到每个层，这样就得到了一个静态量化的模型。

量化时，还有一个关键参数：量化粒度。简单来说，就是量化参数比如scale和offset是针对整个张量所有通道都一样，还是针对每个通道单独计算。

- 逐张量量化，就是一层的所有通道都用同一个scale和offset，简单粗暴，计算量小，但可能损失精度。
- 逐通道量化，每个通道有自己的scale和offset，更精细，能更好地保留通道信息，精度通常会更高，但计算量也稍微大一些，而且有些硬件可能不支持逐通道量化。

我们来看一个具体的静态量化方法：KL散度校准。它的核心思想是，**找到一个最佳的量化阈值，使得量化后的数据分布，尽可能接近原始FP32的分布**。怎么找呢？

- 先收集激活值的直方图，然后尝试不同的截断阈值T，对于每个阈值，计算出量化后的分布
- 用KL散度来衡量这个量化分布和原始分布的差异。KL散度越小，说明量化越好。
- 选择那个让KL散度最小的阈值作为最终的饱和阈值。

在端侧设备上跑量化推理，通常会涉及到几种操作：量化、反量化和重量化。

- 量化是把FP32的数据转换成INT8，比如输入数据和权重。
- 反量化是把INT8的计算结果转换回FP32，比如输出结果。
- 重量化是把INT8的中间结果再量化成INT8，用于下一层的输入。

![端侧量化推理方式](https://chenzomi12.github.io/_images/04PTQ04.png)

最后一些PTQ的实践技巧。量化粒度的选择很重要：

- 对于权重，通常推荐使用逐通道量化，因为不同通道的权重分布差异很大，这样能更好地保留信息。
- 对于激活，通常推荐使用逐张量量化，因为激活在不同通道间差异相对较小，而且逐张量量化能减少异常值的影响，同时计算量也更小。
- 对于残差连接，比如ResNet里的，量化时可能会遇到数据类型冲突的问题，需要特别处理，比如替换一些块，分别量化残差分支，确保数据类型一致。

## 模型剪枝

量化是通过减少数据表示的精度来压缩模型，而模型剪枝则走了一条不同的路：它不改变精度，而是直接动手，把模型中那些不重要的部分给剪掉。剪枝的目标同样是减少模型大小和计算量，但它的优势在于，剪掉后留下的模型往往非常稀疏，也就是很多权重都是零，这在一些专门的硬件上，比如稀疏矩阵加速器，可以实现非常高效的计算。而且，剪枝有时还能意外地提高模型的泛化能力，因为它去除了冗余，让模型更专注于关键特征。

从数学上讲，剪枝可以看作是一个优化问题。我们希望找到一组权重 w，使得模型的损失函数 L 最小，同时还要满足一个约束条件，就是非零权重的数量，也就是 L0 范数，要小于等于一个设定的值 k。这个 k 就是我们想要的稀疏度，也就是模型被压缩的程度。

听起来很完美，但现实是残酷的，因为 L0 范数这个约束，让这个问题变成了一个组合优化问题，是NP-hard的。这意味着，我们不可能在多项式时间内找到绝对最优解，必须依赖一些启发式算法来找到一个足够好的近似解。剪枝之所以能工作，背后有**一个重要的假设：我们训练出来的神经网络，往往是过度参数化的**，也就是说，它包含了很多冗余的参数和连接。剪枝算法就是基于这个假设，试图找出并移除那些不重要的参数，比如权重很小的连接，或者对输出贡献不大的神经元。

不同的剪枝算法，会用不同的策略来判断哪些参数是重要的，哪些是不重要的。比如，有些算法会看权重的大小，有些会看梯度的大小，还有些会结合网络结构来判断。最终的目标都是通过剪枝，让模型变得更精简、更高效。剪枝的方法有很多种，如果按剪枝的粒度来分，可以分为两大类：非结构化和结构化。

- 非结构化剪枝，就像用一把小剪刀，一个一个地剪掉那些看起来不重要的权重，比如绝对值很小的。这种方法非常灵活，压缩率很高，剪掉的权重越多，模型越小。但问题是，剪掉的权重分布很随机，没有规律，一般的硬件很难直接利用这种稀疏性来加速计算。
- 结构化剪枝，则是用更大的剪刀，比如一次性剪掉一个通道、一个滤波器，甚至整个卷积层。这样剪掉后，模型结构还是规整的，部署起来很方便，而且在通用硬件上也能通过一些技巧，比如稀疏矩阵乘法，来实现加速。当然，结构化剪枝的压缩率可能不如非结构化那么高，而且有时候可能会把一些重要的部分也一起剪掉，导致精度损失。

![非结构化和结构化剪枝](https://chenzomi12.github.io/_images/pruning02.png)

这张图更直观地展示了两种剪枝方式的区别。左边是非结构化剪枝，你看那些零值，分布得非常零散，毫无规律。右边是结构化剪枝，比如通道剪枝，就是把整个通道都变成零，或者滤波器剪枝，把整个滤波器都去掉。这样剪掉后，模型的稀疏性就变得非常规整，就像整齐划一的方阵。这种结构化的稀疏性，对于硬件加速来说，就友好多了，因为硬件可以设计专门的电路来高效处理这种结构。

非结构化剪枝虽然听起来很厉害，压缩率高，模型也更稀疏，但它的缺点也很明显。最大的问题就是硬件加速。我们现在的GPU、CPU、NPU，大多是为稠密矩阵设计的，它们很难直接处理这种非规则的稀疏性。虽然理论上可以做稀疏化，但实际效果往往不如预期。另外，非结构化剪枝一旦剪掉一个参数，就很难再恢复。如果后来发现这个参数其实很重要，那模型精度就可能永久下降。所以，非结构化剪枝虽然潜力巨大，但要真正落地，往往需要配合专门的硬件，或者设计一些非常巧妙的算法来弥补它的不足。

当结构化剪枝也有它的局限性，比如压缩率可能不如非结构化那么高，而且有时候，为了保证结构，可能会牺牲一些精度，把一些重要的部分也一起剪掉了。

剪枝操作可以在什么时候进行呢？根据时间点，可以分为四种主要流程。

- 训练前剪枝，简称PBT。这个方法比较激进，它在模型还没开始训练之前，就先根据一些规则，比如随机初始化的权重，直接把一部分参数剪掉，然后用这个稀疏网络去训练。好处是省去了预训练的时间，但缺点是可能剪掉一些重要的参数，影响最终效果。
- 训练中剪枝，简称PDT。这种方法在训练过程中同时进行剪枝，比如通过添加正则化项，或者动态地剪掉一些参数。训练完成后，剪枝也完成了，不需要额外的训练。
- 训练后剪枝，简称PAT。先用标准方法训练一个完整的模型，然后对这个模型进行剪枝，最后再对剪枝后的模型进行微调，让它恢复精度。
- 运行时剪枝，这个比较特殊，它不是在模型部署前就定型，而是在每次推理的时候，根据当前的输入数据，动态地决定哪些部分需要激活，哪些部分可以忽略。

### 训练前剪枝PBT

我们来看训练前剪枝PBT。

![训练前剪枝](https://chenzomi12.github.io/_images/pruning05.png)

这个流程很简单，就像图里展示的，第一步是初始化网络，得到一堆随机的权重。然后，我们根据某种预设的规则，比如随机性，或者基于权重的大小，直接把一部分权重设为零。这样，我们就得到了一个稀疏网络。接下来，我们就直接用这个稀疏网络去训练数据，让它收敛到一个目标精度。这种方法的好处是，省去了预训练的漫长等待，训练和推理都可能更快。但缺点是，我们剪掉的那些参数，可能在训练过程中本来会变得很重要的，这样就可能导致最终的精度不如预期。

### 训练中剪枝PDT

![训练中剪枝](https://chenzomi12.github.io/_images/pruning06.png)

训练中剪枝PDT则更进一步，它把训练和剪枝融合在一起，同步进行。这样训练完成后，剪枝也就完成了，不需要额外的步骤。主要有三种方法：

- 稀疏正则化，比如在损失函数里加上L1正则化，这样模型在训练过程中会倾向于让一些权重变得很小，接近于零。
- 动态稀疏训练，这种方法更复杂，它会先剪掉一些权重，然后又在训练过程中随机长出一些新的权重，不断循环，试图找到一个更优的稀疏结构。
- 评分剪枝，这种方法在训练过程中，实时评估每个参数的重要性，然后直接把那些得分最低的参数剪掉。

总的来说，PDT方法试图让模型在训练过程中就学会如何变得稀疏。

### 训练后剪枝PAT

![训练后剪枝](https://chenzomi12.github.io/_images/pruning07.png)

训练后剪枝PAT，也就是我们常说的Pretrain-Prune-Retrain，是目前最主流、应用最广泛的方法。它的流程是这样的

- 先用标准方法，比如随机梯度下降，把一个完整的、稠密的模型训练好，让它达到一个很高的基准精度。
- 对这个训练好的模型进行剪枝，比如剪掉那些权重很小的连接。
- 剪枝后，模型的精度可能会下降，所以需要进行微调，让它重新学习，恢复精度。
- 还可以评估一下效果，如果还不够满意，可以再回到剪枝步骤，进行迭代优化。

这个流程的好处是，它充分利用了预训练模型学到的丰富知识，剪枝后的模型往往能保持较高的精度。

### 运行时剪枝

这种方法跟前面三种静态剪枝完全不同，它不是在模型部署前就定型，而是非常灵活，它会根据每次输入的数据，动态地决定哪些部分的网络需要计算，哪些部分可以忽略。

比如，对于一个图像分类任务，如果输入的图像很简单，模型可能只需要用到前面几层就能做出判断，后面的深层就可能不重要。运行时剪枝就可以根据这个情况，动态地剪掉后面的网络，只计算前面的部分，从而达到加速的目的。这种方法非常智能，但实现起来也更复杂，因为它需要在每次推理时都进行判断和计算，对推理引擎的效率要求很高。说了这么多剪枝流程，

那具体怎么剪呢？最常用的方法是**基于参数重要性**。那怎么判断重要性呢？有很多种方法。

- 看权重的绝对值，一般来说，权重越接近0，我们认为它对输出的影响越小，就越不重要。
- 看梯度信息，如果一个参数的梯度很小，说明它对损失函数的贡献不大，也可以认为它不重要。
- 做敏感度分析，比如把某个参数的权重设为0，看看模型的精度下降了多少，如果下降得很少，说明这个参数不重要。

## 知识蒸馏

知识蒸馏。这个名字听起来有点玄乎，但其实思想很简单，就是模仿人类的教学过程。我们有一个非常聪明、知识渊博的老师，也就是一个大型的、复杂的神经网络模型，它在海量数据上训练过，积累了丰富的知识。但是，这个老师模型太庞大、太复杂，部署在手机上很慢，占地方。这时候，我们希望有一个聪明的学生，也就是一个小型、轻量的模型，能够快速地把老师的知识学到手，然后自己就能独立工作，而且性能也接近老师。

知识蒸馏就是实现这个目标的一种技术。知识蒸馏系统通常包含三个核心部分：**知识、蒸馏算法和师生架构**。

- 知识，这指的是老师模型里那些有价值的东西，可以是模型的输出，比如分类任务里的概率分布，也可以是模型内部的中间层特征，甚至是模型的参数。
- 蒸馏算法，它规定了学生怎么学习老师，怎么衡量学生学得怎么样，通常会用一个额外的损失函数来衡量学生和老师的差距。
- 师生架构，也就是老师和学生模型的结构，它们是完全一样的吗？还是老师更复杂，学生更简单？它们之间是怎么交互的？这些都是需要考虑的。

老师可以教给学生哪些知识呢？主要有两种。

- 一种是基于响应的知识，也就是老师模型的输出，比如分类任务里，老师输出的类别概率分布，也就是所谓的软标签。这种方法很简单，就是让学生直接模仿老师的输出，让学生的输出也尽可能接近老师。
- 另一种是基于特征的知识，也就是老师模型中间层提取的特征表示。这种方法认为，老师模型学到的中间层特征，包含了更丰富的知识，可以让学生学到更深层次的模式。这种方法通常更适合训练那些比较深的网络。

知识蒸馏的方式也有几种。

- 离线蒸馏，就是老师先教好，然后学生再学。老师模型在训练学生的时候，参数是固定的，就像一个固定的教科书。这种方法的好处是，学生模型训练起来比较简单，部署也方便。但缺点是，学生模型完全依赖于老师，老师如果没教好，学生就没法学。
- 在线蒸馏，老师和学生是同时一起学习的，老师也在不断更新自己的知识，学生也在不断学习。这种方法更像是一种协同学习，可能效果更好，但计算量也更大。
- 自蒸馏，就是老师和学生其实是同一个模型。学生模型自己从自己的预测结果中学习，不需要外部的老师。这种方法可以用来做自监督学习，但可能需要更多的数据或者技巧。